{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "In the introductory lab, we explored ways to perform anomaly detection.  We implemented our own solution to labeling data points that were several standard deviations from the mean of the data points.  Then we estimated linear and seasonal trends in the time series, and removed them to perform anomaly detection on the remaining time series.\n",
    "\n",
    "Here, we are going to apply this approach to our telemetry data.  We will first develop a solution that processes all the data in batch, and then optimize the solution so that it detects anomalies online, as a new sensor measurement arrives.\n",
    "\n",
    "In this lab, we will do the following:\n",
    "- Use python port [pyculiarity](https://github.com/nicolasmiller/pyculiarity) of [Twitter's anomaly detection R library](https://github.com/twitter/AnomalyDetection)\n",
    "- Apply this library to detect anomalies in our telemetry data\n",
    "- Optimize our approach to perform online anomaly detection\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "In order to run this lab, you may have to install these additional python packages (unless you already installed them):\n",
    "- pyculiarity\n",
    "- numpy\n",
    "\n",
    "Do you remember from the previous lab how to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyculiarity import detect_ts # python port of Twitter AD lib\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn # for decent default settings for figures\n",
    "import time # so we can time our operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "As usual, we start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data ... \", end=\"\")\n",
    "telemetry = pd.read_csv(os.path.join('..', 'data', 'telemetry.csv'))\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Parsing datetime...\", end=\"\")\n",
    "telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Reading data ... \", end=\"\")\n",
    "anoms_batch = pd.read_csv(os.path.join('..', 'data', 'anoms.csv'))\n",
    "anoms_batch['datetime'] = pd.to_datetime(anoms_batch['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's pick a random sensor (e.g. volt) and machine id and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 'volt'\n",
    "machine_id = 67\n",
    "\n",
    "# Select the slice of the telemetry for this machine \n",
    "df_s = telemetry.loc[telemetry.loc[:, 'machineID'] == machine_id, ('datetime', sensor)]\n",
    "df_s.columns = ['timestamp', 'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a peak at the data\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "plt.plot(df_s['timestamp'], df_s['value'])\n",
    "df_s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = detect_ts(df_s) # we use the detect_ts function of pyculiarity\n",
    "anoms = results['anoms']\n",
    "\n",
    "plt.plot(df_s['timestamp'], df_s['value'])\n",
    "plt.plot(anoms['timestamp'], anoms['anoms'], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. Looks like we don't really get too many anomalies this way.  We could try to lower our thresholds. Should we try a lower threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2 # The level of statistical significance with which to accept or reject anomalies. (default: 0.05)\n",
    "\n",
    "results = detect_ts(df_s, alpha=alpha)\n",
    "anoms = results['anoms']\n",
    "\n",
    "plt.plot(df_s['timestamp'], df_s['value'])\n",
    "plt.plot(anoms['timestamp'], anoms['anoms'], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't really seem too change to much. We could lower the threshold further, but maybe we need to take a completely different approach.\n",
    "\n",
    "## Denoising data\n",
    "\n",
    "The data look really noisy. Maybe what we need to do is to calculate running averages, because it is not individual measurements that are a problem, but a *collection* of measurements.\n",
    "\n",
    "Let's start by aggregating sensor measurements over the last 12h.  Let's use exponential decay, so that more recent measurements inside the time window have more weight that those that are at the beginning of the time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = 6 # Specify decay in terms of center of mass (this approximates averageing over about twice as many hours)\n",
    "rm = df_s['value'].ewm(com=com).mean()\n",
    "\n",
    "plt.plot(df_s['timestamp'], df_s['value'])\n",
    "plt.plot(df_s['timestamp'], rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, now there seem to be some more apparent anomalies.  Apparently, sometimes sensor readings are off for several hours in a row.  Let's try to run anomaly detection on the smoother time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a data frame with two columns (timestamp, value), which is what pyculiarity expects as input\n",
    "df_smooth = pd.DataFrame(data={'timestamp': df_s['timestamp'], 'value': rm})\n",
    "\n",
    "results = detect_ts(df_smooth)\n",
    "anoms = results['anoms']\n",
    "\n",
    "# we store the location of the last anomaly for later\n",
    "last_anomaly = int(np.where(df_smooth['timestamp'] == results['anoms'].tail(1)['timestamp'].values[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_smooth['timestamp'], df_smooth['value'])\n",
    "plt.plot(anoms['timestamp'], anoms['anoms'], 'o', alpha=.2)\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Sensor Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "\n",
    "That appears to work, but we want our scoring service to work in real-time.  That means we shouldn't be calculating running averages over all the historic data whenever we receive just one new row of data.  Instead we want to use the last existing value of the running average, integrate it with the new data, to get the new running average.  Put another way, we want to calculate running averages *online*, rather than in *batch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating running averages online\n",
    "\n",
    "The first step is to create a lightweight function that can calculate running avgs. \n",
    "\n",
    "[Welford's online](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_Online_algorithm) algorithm for calculating running variance includes a method to also calculate the mean.  This method is also described in Donald Knuth's Art of Computer Programming, Vol 2, page 232, 3rd edition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_avg(ts):\n",
    "    rm_o = np.zeros_like(ts)\n",
    "    rm_o[0] = -1 # enter your solution here\n",
    "\n",
    "    for r in range(1, len(rm)):\n",
    "        curr_com = float(min(com, r))\n",
    "        rm_o[r] = -1 # enter your solution here\n",
    "    return rm_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have trouble coming up with the exact solution, please uncomment below\n",
    "# %load \"../solutions/running_avg.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's confirm that your solution is correct. If your online version produces the same results as the batch version, you should just see an orange line in the below plot.  That is, a perfect blend of red and yellow. If you also see red and yellow, that means your online solution falls short of implmenting the batch version for calculating running averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_o = run_avg(df_s['value'].values)\n",
    "\n",
    "plt.plot(df_smooth['timestamp'], df_smooth['value'], color='yellow')\n",
    "plt.plot(df_smooth['timestamp'], rm_o, alpha=.5, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot he difference between the two resulting time series.  Except for the couple of first values in the timeseries, this should all be zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_smooth['timestamp'], df_smooth['value'] - rm_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Lab\n",
    "\n",
    "## Optimizing the window over which to perform Anomaly Detection\n",
    "\n",
    "One remaining optimization is to only use the last couple of values of the running average for anomaly detection, instead of running it repeatedely on the entire timeseries of the running average.  There is no general answer for how many recent values have to be included to get robust anomaly detection, because the choice of the size of this sliding window depends on all sorts of factors (e.g. machine, sensor, size of anomaly).  \n",
    "\n",
    "This means we have to develop an empirical approach to finding the right size for this sliding window.   \n",
    "\n",
    "The first step is to define a function that performs anomaly detection using only those values that fall within a specified time window leading up to the current value.\n",
    "\n",
    "The smaller the window, the faster the anomaly detection algorithm runs.  However, if we make the window too small, the algorithm will no longer be able to detect linear and seasonal trends, and will fail.\n",
    "\n",
    "Below is a function that allows you to test different window sizes for anomaly detection.  It expects a data frame with two columns (timestamp, value), a specification of the window size, and the row index of an anomaly in the provided data frame.  The function returns whether it found an anomaly and how long it took to run (in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ts_online(df_smooth, window_size, stop):\n",
    "    is_anomaly = False\n",
    "    run_time = 9999\n",
    "    start_index = max(0, stop - window_size)\n",
    "    df_win = df_smooth.iloc[start_index:stop, :]\n",
    "    start_time = time.time()\n",
    "    results = detect_ts(df_win, alpha=0.05, max_anoms=0.02, only_last=None, longterm=False, e_value=False, direction='both')\n",
    "    run_time = time.time() - start_time\n",
    "    if results['anoms'].shape[0] > 0:\n",
    "        timestamp = df_win['timestamp'].tail(1).values[0]\n",
    "        if timestamp == results['anoms'].tail(1)['timestamp'].values[0]:\n",
    "            is_anomaly = True\n",
    "    return is_anomaly, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's run this function to test whether we can recover the last anomaly we previously discovered in this time series using the batch method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_anomaly, run_time = detect_ts_online(df_smooth, 24*7, last_anomaly) \n",
    "\n",
    "print('Success: %s, run_time: %ss' % (is_anomaly, round(run_time,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "\n",
    "We need to systematically explore how different window sizes affect our ability to detect anomalies in the time series.  When we do this, we need to consider both our precision and recall in doing so.  Depending on the operational cost of false positives and negatives, we may have to use different F-measures.\n",
    "\n",
    "In order to do this, we define a function that samples random rows and sensors from the data frame.  It checks whether the batch anomaly detection algorithm detected an anomaly at that time point, and tests whether the online anomaly detection algorithm comes to the same result. \n",
    "\n",
    "It records the number of correct and incorrect anomaly detection results, and calculate accuracy measures.\n",
    "\n",
    "The script accepts two input arguments:\n",
    "- window_size: how much historic data to retain\n",
    "- com: Specify decay in terms of center of mass for running avg\n",
    "\n",
    "\n",
    "We already provided the script for doing this below. But we would like you to discuss with your neighbors which of the following metrics you should use:\n",
    "- precision\n",
    "- recall\n",
    "- f1-score\n",
    "- f2-score (fbeta_score, w/ beta = 2)\n",
    "- f0.5 score\n",
    "\n",
    "You can read up on those in the sklearn documentation of [metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
    "\n",
    "Think about the cost and savings of different settings for the size of the sliding window.  What if the cost of a false negative is high (e.g. USD 50000 for a broken machine), what if that cost is low (e.g. USD 10 for replacing a broken part and restarting the machine)?  What if a false positive is really expensive (e.g. USD 1000 for production loss while machine is being restarted)?\n",
    "\n",
    "To conclude the lab, enter your choice at the bottom of the next script and run it.\n",
    "\n",
    "### End of lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../solutions/sample_run_solution.py\n",
    "\n",
    "def sample_run(df, window_size = 500, com = 12):\n",
    "    \"\"\"\n",
    "    This functions expects a dataframe df as mandatory argument.  \n",
    "    The first column of the df should contain timestamps, the second machine IDs\n",
    "    \n",
    "    Keyword arguments:\n",
    "    n_machines_test: the number of machines to include in the sample\n",
    "    ts_per_machine: the number of timestamps to test for each machine\n",
    "    window_size: the size of the window of data points that are used for anomaly detection\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n_epochs = 10\n",
    "    p_anoms = .5\n",
    "    \n",
    "    # create arrays that will hold the results of batch AD (y_true) and online AD (y_pred)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    run_times = []\n",
    "    \n",
    "    # check which unique machines, sensors, and timestamps we have in the dataset\n",
    "    machineIDs = df['machineID'].unique()\n",
    "    sensors = df.columns[2:]\n",
    "    timestamps = df['datetime'].unique()[window_size:]\n",
    "    \n",
    "    # sample n_machines_test random machines and sensors \n",
    "    random_machines = np.random.choice(machineIDs, n_epochs)\n",
    "    random_sensors = np.random.choice(sensors, n_epochs)\n",
    "\n",
    "    # we intialize an array with that will later hold a sample of timetamps\n",
    "    random_timestamps = np.random.choice(timestamps, n_epochs)\n",
    "    \n",
    "    for i in range(0, n_epochs):\n",
    "        # take a slice of the dataframe that only contains the measures of one random machine\n",
    "        df_s = df[df['machineID'] == random_machines[i]]\n",
    "        \n",
    "        # smooth the values of one random sensor, using our run_avg function\n",
    "        smooth_values = run_avg(df_s[random_sensors[i]].values, com)\n",
    "        \n",
    "        # create a data frame with two columns: timestamp, and smoothed values\n",
    "        df_smooth = pd.DataFrame(data={'timestamp': df_s['datetime'].values, 'value': smooth_values})\n",
    "\n",
    "        # load the results of batch AD for this machine and sensor\n",
    "        anoms_s = anoms_batch[((anoms_batch['machineID'] == random_machines[i]) & (anoms_batch['errorID'] == random_sensors[i]))]\n",
    "                \n",
    "        # find the location of the t'th random timestamp in the data frame\n",
    "        if np.random.random() < p_anoms:\n",
    "            anoms_timestamps = anoms_s['datetime'].values\n",
    "            np.random.shuffle(anoms_timestamps)\n",
    "            counter = 0\n",
    "            while anoms_timestamps[0] < timestamps[0]:\n",
    "                if counter > 100:\n",
    "                    return 0.0, 9999.0\n",
    "                np.random.shuffle(anoms_timestamps)\n",
    "                counter += 1\n",
    "            random_timestamps[i] = anoms_timestamps[0]\n",
    "            \n",
    "        # select the test case\n",
    "        test_case = df_smooth[df_smooth['timestamp'] == random_timestamps[i]]\n",
    "        test_case_index = test_case.index.values[0]\n",
    "\n",
    "        # check whether the batch AD found an anomaly at that time stamps and copy into y_true at idx\n",
    "        y_true_i = random_timestamps[i] in anoms_s['datetime'].values\n",
    "\n",
    "        # perform online AD, and write result to y_pred\n",
    "        y_pred_i, run_times_i = detect_ts_online(df_smooth, window_size, test_case_index)\n",
    "        \n",
    "        y_true.append(y_true_i)\n",
    "        y_pred.append(y_pred_i)\n",
    "        run_times.append(run_times_i)\n",
    "        \n",
    "        score = <put your solution here>\n",
    "        run_time = np.mean(run_times)\n",
    "        \n",
    "    return score, run_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running into issues, please use the following link\n",
    "# %load ../solutions/sample_run_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the parameter space:\n",
    "- Different settings for the size of the sliding window (e.g. 50, 100, 5000, 1000).\n",
    "- Different settigns for the decay in terms of center of mass for running avg (e.g. 4,6,8,12 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "n_epochs = 10\n",
    "window_size = 500\n",
    "com = 12\n",
    "\n",
    "fscore, run_time = sample_run(telemetry, window_size=window_size, n_epochs = n_epochs, com=com)\n",
    "fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning with HyperDrive and BatchAI\n",
    "\n",
    "If you are getting board of trying to find the right hyperparameters, you should stop searching immediately, because after the next lab, we will show you how to use BatchAI to do this tedious labor for you!\n",
    "\n",
    "If you want to, you can already read up on HyperDrive:\n",
    "- [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)\n",
    "- [sample notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/training/03.train-hyperparameter-tune-deploy-with-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
