{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Updating a Docker Image before Deployment as a Webservice\n",
    "\n",
    "This notebook demonstrates how to make changes to an existing docker image, before deploying it as a webservice.  \n",
    "\n",
    "Knowing how to do this can be helpful, for example if you need to debug the execution script of a webservice you're developing, and debugging it involves several iterations of code changes.  In this case it is not an option to deploy your application as a webservice at every iteration, because the time it takes to deploy your service will significantly slow you down.  In some cases, it may be easier to simply run the execution script on the command line, but this not an option if your script accumulates data across individual calls.\n",
    "\n",
    "We describe the following process:\n",
    "\n",
    "1. Configure your Azure Workspace.\n",
    "2. Create a Docker image, using the Azure ML SDK.\n",
    "3. Test your Application by running the Docker container locally.\n",
    "4. Update the execution script inside your running Docker container.\n",
    "5. Commit the changes in your Docker container to its image\n",
    "6. Update the image in the Azure Container Registry (ACR).\n",
    "7. Deploy your Docker image as an Azure Container Instance ([ACI](https://azure.microsoft.com/en-us/services/container-instances/)) Webservice.\n",
    "8. Test your Webservice.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- You need to have an [Azure](https://azure.microsoft.com) subscription. You will also need to know a subscription_id and resource_group.  One way to discover those is to visit the [azure portal](https://portal.azure.com) and look use the same as those of the DSVM.\n",
    "\n",
    "**Note:** \n",
    "- This code was tested on a Data Science Virtual Machine ([DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/)), running Ubuntu Linux 16.04 (Xenial). **Do *not* try to run this notebook on *Databricks***, because you may run into trouble executing some of the shell and docker commands.\n",
    "- If you get an error message when trying to import `azureml` in the first cell below, you probably have to switch to using the correct kernel: `Python 3.6 AzureML`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your Azure Workspace\n",
    "\n",
    "We need to set up your workspace, and make sure we have access to it from here.\n",
    "\n",
    "This requires that you have downloaded the `config.json` configuration file for your azure workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "from azureml.core import Workspace\n",
    "\n",
    "# ws = Workspace.from_config()\n",
    "\n",
    "subscription_id = \"\"\n",
    "resource_group = \"\"\n",
    "workspace_name = \"my_mlads_ws\"\n",
    "workspace_region = \"westus2\"\n",
    "\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,\n",
    "                      exist_ok=True)\n",
    "\n",
    "ws.get_details()\n",
    "\n",
    "# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that you have the correction version of the Azure ML SDK installed on your workstation or VM.  If you don't have the write version, please follow these [Installation Instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python#install-the-sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check version"
    ]
   },
   "outputs": [],
   "source": [
    "import azureml\n",
    "\n",
    "# display the core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "\n",
    "if azureml.core.VERSION == '0.1.59':\n",
    "    print(\"Looks like you have the correct version. We are good to go.\")\n",
    "else:\n",
    "    print(\"There is a version mismatch, this notebook may not work as expected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker image using the Azure ML SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a template execution script for your application\n",
    "\n",
    "We are going to start with just a barebones execution script for your webservice.  This script is only for testing, and will do nothing but thank us for interacting with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import json # we use json in order to interact with the anomaly detection service via a RESTful API\n",
    "\n",
    "# The init function is only run once, when the webservice (or Docker container) is started\n",
    "def init():\n",
    "    global running_avg, curr_n\n",
    "    \n",
    "    running_avg = 0.0\n",
    "    curr_n = 0\n",
    "    \n",
    "    pass\n",
    "\n",
    "# the run function is run everytime we interact with the service\n",
    "def run(raw_data):\n",
    "    \"\"\"\n",
    "    Calculates rolling average according to Welford's online algorithm.\n",
    "    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online\n",
    "\n",
    "    :param raw_data: raw_data should be a json query containing a dictionary with the key 'value'\n",
    "    :return: runnin_avg (float, json response)\n",
    "    \"\"\"\n",
    "    global running_avg, curr_n\n",
    "    \n",
    "    value = json.loads(raw_data)['value']\n",
    "    n_arg = 5 # we calculate the average over the last \"n\" measures\n",
    "    \n",
    "    curr_n += 1\n",
    "    n = min(curr_n, n_arg) # in case we don't have \"n\" measures yet\n",
    "    \n",
    "    running_avg += (value - running_avg) / n\n",
    "    \n",
    "    return json.dumps(running_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment file for your Conda environment\n",
    "\n",
    "Next, create an environment file (environment.yml) that specifies all the python dependencies of your script. This file is used to ensure that all of those dependencies are installed in the Docker image.  Let's assume your Webservice will require ``azureml-sdk``, ``scikit-learn``, and ``pynacl``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "set conda dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "myenv.add_pip_package(\"pynacl==1.2.1\")\n",
    "myenv.add_pip_package(\"pyculiarity\")\n",
    "myenv.add_pip_package(\"pandas\")\n",
    "myenv.add_pip_package(\"numpy\")\n",
    "\n",
    "with open(\"environment.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of the `environment.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"environment.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the initial Docker image\n",
    "\n",
    "We use the ``environment.yml`` and ``score.py`` files from above, to create an initial Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "# configure the image\n",
    "image_config = ContainerImage.image_configuration(execution_script = \"score.py\", \n",
    "                                                  runtime = \"python\",\n",
    "                                                  conda_file = \"environment.yml\")\n",
    "\n",
    "# create the docker image. this should take less than 5 minutes\n",
    "image = ContainerImage.create(name = \"my-docker-image\",\n",
    "                              image_config = image_config,\n",
    "                              models = [],\n",
    "                              workspace = ws)\n",
    "\n",
    "# we wait until the image has been created\n",
    "image.wait_for_creation(show_output=True)\n",
    "\n",
    "# let's save the image location\n",
    "imageLocation = image.serialize()['imageLocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your Application by running the Docker container locally\n",
    "\n",
    "### Download the created Docker image from the Azure Container Registry ([ACR](https://azure.microsoft.com/en-us/services/container-registry/))\n",
    "\n",
    "Here we use some [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) to exchange variables between python and bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imageLocation\" \n",
    "\n",
    "# get the location of the docker image in ACR\n",
    "imageLocation=$1\n",
    "\n",
    "# extract the address of the repository within ACR\n",
    "repository=$(echo $imageLocation | cut -f 1 -d \".\")\n",
    "\n",
    "echo \"Attempting to login to repository $repository\"\n",
    "az acr login --name $repository\n",
    "echo\n",
    "\n",
    "echo \"Trying to pull image $imageLocation\"\n",
    "docker pull $imageLocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the docker container\n",
    "\n",
    "We use standard Docker commands to start the container locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imageLocation\"\n",
    "\n",
    "# extract image name and tag from imageLocation\n",
    "image_name=$(echo $1 | cut -f 1 -d \":\")\n",
    "tag=$(echo $1 | cut -f 2 -d \":\")\n",
    "echo \"Image name: $image_name, tag: $tag\"\n",
    "\n",
    "# extract image ID from list of downloaded docker images\n",
    "image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n",
    "echo \"Image ID: $image_id\"\n",
    "\n",
    "# we forward TCP port 5001 of the docker container to local port 8080 for testing\n",
    "echo \"Starting docker container\"\n",
    "docker run -d -p 8080:5001 $image_id\n",
    "\n",
    "sleep 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the docker container\n",
    "\n",
    "We test the docker container, by sending some data to it to see how it responds - just as we would with a Webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values = np.random.normal(0,1,100)\n",
    "values = np.cumsum(values)\n",
    "\n",
    "\n",
    "running_avgs = []\n",
    "\n",
    "for value in values:\n",
    "    raw_data = {\"value\": value}\n",
    "\n",
    "    r = requests.post('http://localhost:8080/score', json=raw_data)\n",
    "\n",
    "    result = json.loads(r.json())\n",
    "    running_avgs.append(result)\n",
    "\n",
    "plt.plot(values)\n",
    "plt.plot(running_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the container\n",
    "\n",
    "Let's make a change to the the execution script: Copy the changed ``AD_score.py`` into the running docker container and commit the changes to the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $imageLocation\n",
    "\n",
    "image_location=$1\n",
    "\n",
    "# extract image name and tag from imageLocation\n",
    "image_name=$(echo $image_location | cut -f 1 -d \":\")\n",
    "tag=$(echo $image_location | cut -f 2 -d \":\")\n",
    "\n",
    "echo \"Image name: $image_name, tag: $tag\"\n",
    "\n",
    "# extract image id\n",
    "image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n",
    "\n",
    "echo \"Image ID: $image_id\"\n",
    "\n",
    "# extract container ID\n",
    "container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n",
    "echo \"Container ID: $container_id\"\n",
    "\n",
    "# copy modified scoring script again\n",
    "docker cp AD_score.py $container_id:/var/azureml-app/score.py\n",
    "sleep 1\n",
    "\n",
    "# stop the container\n",
    "docker restart $container_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load telemetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "print(\"Reading data ... \", end=\"\")\n",
    "telemetry = pd.read_csv(os.path.join('..', 'data', 'telemetry.csv'))  # , parse_dates=['datetime'])\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Parsing datetime...\", end=\"\")\n",
    "telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "telemetry.columns = ['timestamp', 'machineID', 'volt', 'rotate', 'pressure', 'vibration']\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "def test_docker(telemetry, n=None):\n",
    "    \"\"\"\n",
    "        n is the number of sensor readings we are simulating\n",
    "        \"\"\"\n",
    "\n",
    "    if not n:\n",
    "        n = telemetry.shape[0]\n",
    "\n",
    "    machine_ids = [1] # telemetry['machineID'].unique()\n",
    "    timestamps = telemetry['timestamp'].unique()\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    for timestamp in timestamps[:10]:\n",
    "        np.random.shuffle(machine_ids)\n",
    "        for machine_id in machine_ids:\n",
    "            data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n",
    "            json_data = data.to_json()\n",
    "            input_data = {\"data\": json_data}\n",
    "\n",
    "            r = requests.post('http://localhost:8080/score', json=input_data)\n",
    "\n",
    "            result = pd.read_json(json.loads(r.json()))\n",
    "            out_df = out_df.append(result, ignore_index=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing ... \")\n",
    "out_df = test_docker(telemetry)\n",
    "print(\"Done.\")\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = telemetry['timestamp'].unique()\n",
    "timestamp = timestamps[0]\n",
    "machine_id = 1\n",
    "data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n",
    "json_data = json.dumps(data.to_json())\n",
    "json_data\n",
    "new_data = pd.read_json(json.loads(json_data))\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 2  # set the number of values going into the running avg\n",
    "values = np.random.normal(0,1,100)\n",
    "values = np.cumsum(values)\n",
    "\n",
    "\n",
    "running_avgs = []\n",
    "\n",
    "for value in values:\n",
    "    raw_data = {\"value\": value, \"n\": n}\n",
    "\n",
    "    r = requests.post('http://localhost:8080/score', json=raw_data)\n",
    "\n",
    "    result = json.loads(r.json())\n",
    "    running_avgs.append(result)\n",
    "\n",
    "plt.plot(values)\n",
    "plt.plot(running_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the updated container to ACR\n",
    "\n",
    "**First**, test your Docker container again (run the json query above), to ensure that the changes are having the expected effect. \n",
    "\n",
    "**Then** you can push the image into ACR, so that it can be retrieved by the Azure ML SDK when you want to deploy your Webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imageLocation\"\n",
    "\n",
    "image_location=$1\n",
    "\n",
    "# extract container ID\n",
    "container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n",
    "echo \"Container ID: $container_id\"\n",
    "\n",
    "# commit changes made in the container to the local copy of the image\n",
    "docker commit $container_id $image_location\n",
    "\n",
    "docker push $image_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to deploy the container to ACI, just to make sure everything behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# create configuration for ACI\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"some data\",  \"method\" : \"machine learning\"}, \n",
    "                                               description=\"Does machine learning on some data\")\n",
    "# pull the image\n",
    "image = ContainerImage(ws, name='my-docker-image', tags='1')\n",
    "\n",
    "# deploy webservice\n",
    "service_name = 'my-web-service'\n",
    "service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                            image = image,\n",
    "                                            name = service_name,\n",
    "                                            workspace = ws)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # set the number of values going into the running avg\n",
    "values = np.random.normal(0,1,100)\n",
    "values = np.cumsum(values)\n",
    "\n",
    "\n",
    "running_avgs = []\n",
    "\n",
    "for value in values:\n",
    "    raw_data = json.dumps({\"value\": value, \"n\": n})\n",
    "    raw_data = bytes(raw_data, encoding = 'utf8')\n",
    "    \n",
    "    # predict using the deployed model\n",
    "    result = json.loads(service.run(input_data=raw_data))\n",
    "\n",
    "    running_avgs.append(result)\n",
    "\n",
    "plt.plot(values)\n",
    "plt.plot(running_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "To keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "msauthor": "sgilley"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
