{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Anomaly Detection\n",
    "\n",
    "This notebook provides a short primer on Anomaly Detectin in time series data. \n",
    "\n",
    "The goal is to provide you an intuitive understanding of the algorithmic approach we are going to be using for anomaly detection.\n",
    "\n",
    "We will try to achieve this goal using the following steps:\n",
    "1. Create a hypothetical time series dataset\n",
    "2. Apply simple, but common anomaly detection\n",
    "3. Understand the limitations of this approach in dealing with\n",
    "    - seasonal trends (e.g. higher energy consumption on the weekends than on weekdays)\n",
    "    - linear trends (e.g. growth over time)\n",
    "4. Improve the anomaly detection algorithm to be robust against seasonal and linear trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's start with loading python modules we need for this course\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "from rstl import STL\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn\n",
    "import numpy as np\n",
    "\n",
    "wide_figure = (16,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a hypothetical time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_measures = 24 * 365 # we simulate one measurement per hour for one year\n",
    "\n",
    "# define nature of anomalies\n",
    "p_anom = 0.001 # probability of anomalies\n",
    "loc, scale = 5, .1 # we sample anomalies from a normal distribution with mean 'loc' and std 'scale'\n",
    "\n",
    "x = np.linspace(1, n_measures, n_measures)\n",
    "x_tics = x / 24 # we plot x-axis in days\n",
    "\n",
    "# let's create a time series normal data, by drawing independent samples from a normal distribution\n",
    "y_reg = np.random.normal(size=int(round(n_measures * (1 - p_anom)))).tolist()\n",
    "\n",
    "# anomalies are also drawn from a normal distribution, but with a different mean and standard deviation\n",
    "y_anom = np.random.normal(loc=loc, scale=scale, size=int(round(n_measures * (p_anom)))).tolist()\n",
    "\n",
    "# we concatentate the two lists into an array, and shuffle the array\n",
    "y = np.array(y_reg + y_anom)\n",
    "np.random.shuffle(y)\n",
    "\n",
    "# let's see what this looks like\n",
    "fig = plt.figure(figsize=wide_figure)\n",
    "plt.plot(x_tics,y)\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Telemetry (Measurement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Approaches to Anomaly detection\n",
    "\n",
    "Let's start with one of the simplest approaches to anomaly detection:\n",
    "1. We calculate the mean and the standard deviation of all instances\n",
    "2. We mark every instances that is too far from the mean as an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 4 # how many standard deviations does an instance have to be from the mean in order to be an anomaly\n",
    "\n",
    "m = np.mean(y)\n",
    "std = np.std(y)\n",
    "\n",
    "# find the array indices of extreme values (anomalies)\n",
    "idx = np.where(y > m + tolerance * std)[0].tolist()\n",
    "\n",
    "# create an array that is all NaN, except for the anomalies\n",
    "anoms = np.full(y.shape[0], np.nan)\n",
    "anoms[idx] = y[idx] # copy the value of the anomaly\n",
    "anoms_orig = np.array(anoms) # let's store a copy of the anomalies for later\n",
    "\n",
    "fig = plt.figure(figsize=wide_figure)\n",
    "plt.plot(x_tics, y)\n",
    "plt.plot(x_tics, [m + tolerance * std] * len(x_tics), '--')\n",
    "plt.plot(x_tics, anoms, 'o')\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Telemetry (Measurement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "\n",
    "This lab is mainly a Python programming exercise.\n",
    "\n",
    "Can you write a function that accepts a time series as input, detects the anomalies, and returns a numpy array, as we did above.  The function should also accept a `keyword argument` to specify the `tolerance` of the anomaly detection.  This exercise will ensure you understand how we defined anomalies and offer you a chance to practice your Python skills.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(<arguments>):\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "    return anoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are not able to implement the solution above, uncomment the below line and run this cell (twice)\n",
    "# %load ../solutions/basic_anom_detect.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your code. Are the results the same as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms = detect_anomalies(y)\n",
    "\n",
    "fig = plt.figure(figsize=wide_figure)\n",
    "plt.plot(x_tics, y)\n",
    "plt.plot(x_tics, anoms, 'o')\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Telemetry (Measurement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of this approach\n",
    "\n",
    "Looks like this worked really well, but what if you use some more realistic data. Real-world data often has seasonal or linear trends.  Cars tend to consume more oil per mile when they get older.  Power consumption is typically higher on weekdays than on weekends, when office buildings are empty.\n",
    "\n",
    "### Seasonal Trends\n",
    "\n",
    "Let's see how this approach performs if the data have some seasonality: weekly cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 24 * 7 * 4 # for display purposes\n",
    "\n",
    "# seasonal trend (weekly)\n",
    "amp = 1.5 # amplitude of the weekly effect\n",
    "mod = amp * np.cos(2 * np.pi * x / 24 / 7)\n",
    "y_mod = y + mod # create a new timeseries by adding the seasonal effect to the original data\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(14,8))\n",
    "ax[0].plot(x_tics[:sample_size], y[:sample_size])\n",
    "ax[1].plot(x_tics[:sample_size], mod[:sample_size])\n",
    "ax[2].plot(x_tics[:sample_size], y_mod[:sample_size])\n",
    "ax[0].set_xlabel('Time (days)')\n",
    "ax[0].set_ylabel(\"Telemetry (Measurement)\")\n",
    "ax[1].set_title('weekly cycle')\n",
    "ax[2].set_title('original data + weekly cycle')\n",
    "ax[0].set_title('original data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same approach as before and see how it performs on this new time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms = detect_anomalies(y_mod)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,8))\n",
    "\n",
    "ax[0].plot(x_tics, y_mod)\n",
    "ax[0].plot(x_tics, anoms, 'o')\n",
    "ax[0].set_xlabel(\"Days\")\n",
    "ax[0].set_ylabel(\"Telemetry (Measurement)\")\n",
    "ax[0].set_title(\"original data + weekly cycle\")\n",
    "\n",
    "ax[1].plot(x_tics, y)\n",
    "ax[1].plot(x_tics, anoms_orig, 'o')\n",
    "ax[1].set_xlabel('Time (days)')\n",
    "ax[1].set_title(\"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our approach was still able to recover some of the anomalies, but this would clearly not be acceptable.  Imagine you have a situation where not detecting an anomaly could lead to massive costs due to equipment damage!\n",
    "\n",
    "### Linear trends\n",
    "\n",
    "Let's add a linear trend to our data to see how the algorithm performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = .001 # slope of the linear trend\n",
    "linear_trend = x * slope\n",
    "y_mod_lin = y_mod + linear_trend\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(14,8), sharey=True)\n",
    "ax[0].plot(x_tics[:sample_size], y_mod[:sample_size])\n",
    "ax[1].plot(x_tics[:sample_size], linear_trend[:sample_size])\n",
    "ax[2].plot(x_tics[:sample_size], y_mod_lin[:sample_size])\n",
    "ax[0].set_xlabel('Time (days)')\n",
    "ax[0].set_title('original + weekly cycle')\n",
    "ax[1].set_title('linear trend')\n",
    "ax[2].set_title('original + weekly cycle + linear trend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and see how it does. If you don't see any anomalies, try lowering the `tolerance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 4\n",
    "\n",
    "anoms = detect_anomalies(y_mod_lin, tolerance=tolerance)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,8))\n",
    "\n",
    "ax[0].plot(x_tics, y_mod_lin)\n",
    "ax[0].plot(x_tics, anoms, 'o')\n",
    "ax[0].set_xlabel('Time (days)')\n",
    "ax[0].set_title('orig + weekly cycle + linear trend')\n",
    "\n",
    "ax[1].plot(x_tics, y)\n",
    "ax[1].plot(x_tics, anoms_orig, 'o')\n",
    "ax[1].set_xlabel('Time (days)')\n",
    "ax[1].set_title(\"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Did it work? You could try lowering the `tolerance` (e.g. 2). Did that help?\n",
    "\n",
    "It looks like if we have seasonal and linear trends in our time series, the simple approach from above doesn't work at all!\n",
    "\n",
    "Let's see whether we can fix that.\n",
    "\n",
    "## Improve the anomaly detection algorithm to be robust against seasonal and linear trends\n",
    "\n",
    "One common approach to this dilema is STL, a versatile and robust method for decomposing time series. STL is an acronym for \"Seasonal and Trend decomposition using Loess\". \n",
    "\n",
    "The STL method was developed by Cleveland, Cleveland, McRae, & Terpenning (1990).\n",
    "\n",
    "STL has several strengths:\n",
    "- It handles *any type* of seasonality, including monthly, weekly, daily seasonality.\n",
    "- The seasonal component is allowed to change over time, and the rate of change can be controlled by the user.\n",
    "- The smoothness of the trend-cycle can also be controlled by the user.\n",
    "- It can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components.\n",
    "\n",
    "Let's give it a try.\n",
    "\n",
    "The STL decomposition method accepts a time series as input, and decomposes the time series into a linear trend, seasonal trend, and a remainder.  Put another way, the remainder is what is left if we subtract the linear and seasonal trends from the original time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl = STL(y_mod_lin, freq=24*7, s_window='periodic')\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(16,8))\n",
    "ax[0].plot(x, y_mod_lin) # the time series\n",
    "ax[0].set_title(\"original + linear + seasonal trend\")\n",
    "ax[1].plot(x, stl.trend) # linear trend\n",
    "ax[1].set_title(\"linear trend\")\n",
    "ax[2].plot(x, stl.seasonal) # sesonal trend\n",
    "ax[2].set_title(\"seasonal trend\")\n",
    "ax[3].plot(x, stl.remainder) # remainder\n",
    "ax[3].set_title(\"remainder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try whether we can apply our approach to anomaly detection. \n",
    "\n",
    "If STL did its job, the `remainder` should look just like the original time series, before we added linear and seasonal trends. So if we run our original approach to anomaly detection, we should be able to detect the same anomalies on the remainder after running STL decomposition as we did on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms = detect_anomalies(stl.remainder)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,8))\n",
    "ax[0].plot(x_tics, stl.remainder)\n",
    "ax[0].plot(x_tics, anoms, 'o')\n",
    "ax[0].set_xlabel('Time (days)')\n",
    "ax[0].set_title(\"stl.remainder\")\n",
    "\n",
    "ax[1].plot(x_tics, y)\n",
    "ax[1].plot(x_tics, anoms_orig, 'o')\n",
    "ax[1].set_xlabel('Time (days)')\n",
    "ax[1].set_title(\"original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we were able to recover all the anomalies!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By now you should have an intuitive understanding of the approach to anomaly detection we will follow in this lab. \n",
    "\n",
    "We will follow a divide and conquer strategy, of first removing linear and seasonal trends, so that we can simply mark all extreme values as anomalies.\n",
    "\n",
    "In the process of this notebook you were also introduced to the following:\n",
    "- How to generate a dataset for exploring your idea.  Using an idealized dataset can be very helpful, because you know exactly what the result should look like.  For example, in our case we knew that we were successful if could discover the same anomalies as the ones that we had discovered orignally.\n",
    "- A simple, but common approach to detecting point anomalies.\n",
    "- The STL algorithm, a very powerful tool that can be used even outside of anomaly detection, as part of data preparation or preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on lab\n",
    "\n",
    "This lab is mainly a Python programming exercise.\n",
    "\n",
    "Can you update your function for anomaly detection from above to perform STL first, before applying anomaly detection?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_stl(<arguments>):\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "    return anoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyculiarity_p3]",
   "language": "python",
   "name": "conda-env-pyculiarity_p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
